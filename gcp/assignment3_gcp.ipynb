{"cells":[{"cell_type":"code","execution_count":185,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n","cluster-0aea  GCE       2                                             RUNNING  us-central1-a\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":187,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":188,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":188,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import hashlib\n","from nltk.util import ngrams\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","from nltk.stem import WordNetLemmatizer\n","from math import log10\n","from inverted_index_gcp import InvertedIndex\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":189,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-0aea-m.c.final-project-415618.internal:45751\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f4d940ab490>"]},"execution_count":189,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":190,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'tfidf_bucket_318437159' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Getting the data"]},{"cell_type":"code","execution_count":191,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/02 19:20:25 WARN SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# adding our python module to the cluster\n","%cd -q /home/dataproc\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"72bcf46a"},"source":["# Building an inverted index"]},{"cell_type":"code","execution_count":192,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","SIZE_OF_WIKI = 6348910\n","LONGEST_WIKI_ARTICLE = 17659\n","\n","stemmer = PorterStemmer()\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\",  \"make\",\n","                    \"accordingly\", \"hence\",\n","    \"namely\",\n","    \"therefore\",\n","    \"thus\",\n","    \"consequently\",\n","    \"meanwhile\",\n","    \"accordingly\",\n","    \"likewise\",\n","    \"similarly\",\n","    \"notwithstanding\",\n","    \"nonetheless\",\n","    \"despite\",\n","    \"whereas\",\n","    \"furthermore\",\n","    \"moreover\",\n","    \"nevertheless\",\n","    \"although\",\n","    \"notably\",\n","    \"notwithstanding\",\n","    \"nonetheless\",\n","    \"despite\",\n","    \"whereas\",\n","    \"furthermore\",\n","    \"moreover\",\n","    \"notably\", \"hence\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","\n","def word_count(text, id, biagram, use_doc_len):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in\n","    `all_stopwords` and return entries that will go into our posting lists.\n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    tokens = [tok for tok in tokens if tok not in all_stopwords]\n","\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    if biagram:\n","        tokens = list(ngrams(tokens, 2))\n","        tokens = [' '.join(bigram) for bigram in tokens]\n","    counter = Counter(tokens)\n","    normalizer = counter.most_common(1)[0][1] if counter else 1\n","    if use_doc_len:\n","        normalizer = len(tokens) \n","    return [(token, (id, count/normalizer)) for token, count in counter.items()]\n","\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","    list of tuples\n","      A sorted posting list.\n","    '''\n","    return sorted(unsorted_pl, key=lambda x: x[0])\n","\n","# def add_tfIDF(postingList):\n","#     word_freq = len(postingList)\n","#     newPost = []\n","#     for post in postingList:\n","#         newPost.append((post[0], post[1]*log10(SIZE_OF_WIKI / word_freq)))\n","#     return newPost\n","\n","def calculate_df(postings):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","      postings: RDD\n","        An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","      RDD\n","        An RDD where each element is a (token, df) pair.\n","    '''\n","    # Count the document frequency for each token\n","    return postings.map(lambda x: (x[0], len(x[1])))\n","\n","def partition_postings_and_write(postings, folder_name):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # posting in postings is word and posting list tuple\n","    bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), x))\n","    # Group postings by bucket ID\n","    grouped_postings = bucketed_postings.groupByKey()\n","    # Write each bucket's postings to disk and collect locations\n","    posting_locations = grouped_postings.map(lambda x: InvertedIndex.write_a_posting_list(x, folder_name, bucket_name))\n","\n","    return posting_locations"]},{"cell_type":"code","execution_count":193,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def create_posting_locs(doc_text_pairs, folder_name, stemOrBiagrm, doc_len, filter_thershold):\n","    # word counts map\n","    word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1], stemOrBiagrm, doc_len))\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    postings = postings.mapValues(add_tfIDF)# filtering postings and calculate df\n","    postings_filtered = postings.filter(lambda x: len(x[1]) > filter_thershold)\n","    w2df = calculate_df(postings_filtered)\n","    w2df_dict = w2df.collectAsMap()\n","    # partition posting lists and write out\n","    partition_postings_and_write(postings_filtered, folder_name).collect()\n","    return w2df_dict\n","    "]},{"cell_type":"code","execution_count":194,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def read_posting_locs(folder_name):\n","    # collect all posting lists locations into one super-set\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix=f'{folder_name}'):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","            for k, v in posting_locs.items():\n","                super_posting_locs[k].extend(v)\n","    return super_posting_locs"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"VhAV0A6dNZWY"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":195,"id":"0b8efa3e","metadata":{},"outputs":[],"source":["def create_posting_map(super_posting_locs, w2df_dict, inverted_name):    \n","    # Create inverted index instance\n","    inverted = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    inverted.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    inverted.df = w2df_dict\n","    # write the global stats out\n","    inverted.write_index('.', inverted_name)\n","    # upload to gs\n","    index_src = \"index.pkl\"\n","    index_dst = f'gs://{bucket_name}/inverted_indices/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    return inverted"]},{"cell_type":"code","execution_count":196,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[],"source":["def create_invertedIndex(doc_text_pairs, folder_name, Biagram, doc_len, filter_thershold):\n","    w2df_dict = create_posting_locs(doc_text_pairs, folder_name, Biagram, doc_len, filter_thershold)\n","    super_posting_locs = read_posting_locs(folder_name)\n","    return create_posting_map(super_posting_locs, w2df_dict, folder_name)\n"]},{"cell_type":"code","execution_count":null,"id":"7fcaa843","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 547:>                                                        (0 + 1) / 1]\r"]}],"source":["STEMMING_BODY_FOLDER = 'inverted_test_text_with_stemming'\n","BIGRAM_BODY_FOLDER = 'inverted_test_text_with_bigram'\n","STEMMING_TITLE_FOLDER = 'inverted_test_title_with_stemming'\n","BIGRAM_TITLE_FOLDER = 'inverted_test_title_with_bigram'\n","STEMMING_BODY_MOST_COMMON_FOLDER = 'inverted_test_body_most_common_with_stemming'\n","STEMMING_TITLE_MOST_COMMON_FOLDER = 'inverted_test_title_most_common_with_stemming'\n","\n","parquetFile = spark.read.parquet(*paths)\n","text_data = parquetFile.limit(10000).select(\"text\", \"id\").rdd  \n","title_data = parquetFile.limit(10000).select(\"title\", \"id\").rdd  \n","inverted1 = create_invertedIndex(text_data, f'{STEMMING_BODY_FOLDER}', False, True, 10)\n","inverted3 = create_invertedIndex(title_data, f'{STEMMING_TITLE_FOLDER}', False , True, 4)\n","text_data = parquetFile.limit(10000).select(\"text\", \"id\").rdd  \n","title_data = parquetFile.limit(10000).select(\"title\", \"id\").rdd  \n","inverted2 = create_invertedIndex(text_data, f'{BIGRAM_BODY_FOLDER}', True , True, 10)\n","inverted4 = create_invertedIndex(title_data, f'{BIGRAM_TITLE_FOLDER}', True , True, 4)\n","text_data = parquetFile.limit(10000).select(\"text\", \"id\").rdd  \n","title_data = parquetFile.limit(10000).select(\"title\", \"id\").rdd  \n","inverted5 = create_invertedIndex(text_data, f'{BIGRAM_BODY_MOST_COMMON_FOLDER}', True , False, 10)\n","inverted6 = create_invertedIndex(title_data, f'{BIGRAM_TITLE_MOST_COMMON_FOLDER}', True, False, 4)"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"fc0667a9","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# PageRank"]},{"cell_type":"code","execution_count":173,"id":"76f87e51","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'high school': 79}\n"]}],"source":["print(inverted6.df)\n"]},{"cell_type":"code","execution_count":78,"id":"31a516e2","metadata":{"id":"yVjnTvQsegc-"},"outputs":[],"source":["def generate_graph(pages):\n","    vertices = pages.flatMap(lambda page: [(page[0], )] + [(link[0], ) for link in page[1]])\n","\n","    edges = pages.flatMap(lambda page: [(page[0], link[0]) for link in page[1]])\n","\n","    edges = edges.distinct()\n","    vertices = vertices.distinct()\n","\n","    return edges, vertices"]},{"cell_type":"code","execution_count":79,"id":"6bc05ba3","metadata":{"id":"db005700","nbgrader":{"grade":false,"grade_id":"cell-PageRank","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n","  warnings.warn(\n","/usr/lib/spark/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n","  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n","[Stage 340:================================================>    (113 + 2) / 124]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+------------------+\n","|     id|          pagerank|\n","+-------+------------------+\n","|  68253| 404.5411250314113|\n","|  41512| 403.9501793282662|\n","|1234326|  403.837684837305|\n","| 382207|403.32764088914206|\n","|  53509|393.25732447614223|\n","| 345173| 246.6592577008005|\n","|  44812|206.22810908594528|\n","| 344869|  102.311634129927|\n","|  74594|  65.2200954494868|\n","|  20593|56.835850990358374|\n","|  16815| 51.92827670733025|\n","|6769081|30.796224517795995|\n","|6650649| 29.07635029356804|\n","|6613141|28.476027061310425|\n","| 386690| 25.34798375347078|\n","| 386720| 24.06624301563353|\n","|7036474|  22.3572316936504|\n","| 386723|21.977433179754335|\n","|6662872| 20.88566477845661|\n","| 171166|20.270344350725992|\n","+-------+------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["t_start = time()\n","pages_links = parquetFile.limit(10000).select(\"id\",\"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n","pr_time = time() - t_start\n","pr.show()"]},{"cell_type":"code","execution_count":184,"id":"747cb66b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 541:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["15013\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["def filter_stop_words(text):\n","\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    tokens = [tok for tok in tokens if tok not in all_stopwords]\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    tokens = list(ngrams(tokens, 2))\n","    return [' '.join(bigram) for bigram in tokens]\n","\n","\n","title_data = parquetFile.limit(10000).select(\"title\", \"id\").rdd\n","word_counts = title_data.flatMap(lambda text: filter_stop_words(text[0]))\n","\n","print(word_counts.count())\n","               "]},{"cell_type":"code","execution_count":null,"id":"57a6adac","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"cb6677f7","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}