{"cells":[{"cell_type":"code","execution_count":2,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME                     PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","lebron-the-cluster-c3e8  GCE       2                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":3,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","# !pip install gensim"]},{"cell_type":"code","execution_count":4,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import hashlib\n","from nltk.util import ngrams\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","from nltk.stem import WordNetLemmatizer\n","from math import log10\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":5,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'lebron_the_bucket_208627935' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and b.name.endswith('.parquet'):\n","        paths.append(full_path+b.name)"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Getting the data"]},{"cell_type":"code","execution_count":6,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":false},"outputs":[],"source":["# adding our python module to the cluster\n","%cd -q /home/dataproc\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"5540c727","metadata":{"id":"72bcf46a"},"source":["# Building an inverted index"]},{"cell_type":"code","execution_count":6,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","stemmer = PorterStemmer()\n","english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\",  \"make\",\n","                    \"accordingly\", \"hence\", \"namely\", \"therefore\", \"thus\",\n","                    \"consequently\",\n","                    \"meanwhile\",\n","                    \"accordingly\",\n","                    \"likewise\",\n","                    \"similarly\",\n","                    \"notwithstanding\",\n","                    \"nonetheless\",\n","                    \"despite\",\n","                    \"whereas\",\n","                    \"furthermore\",\n","                    \"moreover\",\n","                    \"nevertheless\",\n","                    \"although\",\n","                    \"notably\",\n","                    \"notwithstanding\",\n","                    \"nonetheless\",\n","                    \"despite\",\n","                    \"whereas\",\n","                    \"furthermore\",\n","                    \"moreover\",\n","                    \"notably\", \"hence\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","NUM_BUCKETS = 124\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def tokenize_text(text, biagram, stem):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    tokens = [tok for tok in tokens if tok not in all_stopwords]\n","    if stem:\n","        tokens = [stemmer.stem(token) for token in tokens]   \n","    if biagram:\n","        tokens = list(ngrams(tokens, 2))\n","        tokens = [' '.join(bigram) for bigram in tokens]   \n","    return tokens\n","\n","def word_count(tokens_of_text, id):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in\n","    `all_stopwords` and return entries that will go into our posting lists.\n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs\n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","    '''\n","    counter = Counter(tokens_of_text) \n","    return [(token, (id, count)) for token, count in counter.items() if count > 10]\n","\n","def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples\n","    Returns:\n","    --------\n","    list of tuples\n","      A sorted posting list.\n","    '''\n","    return sorted(unsorted_pl, key=lambda x: x[0])\n","\n","# def add_tfIDF(postingList):\n","#     word_freq = len(postingList)\n","#     newPost = []\n","#     for post in postingList:\n","#         newPost.append((post[0], post[1]*log10(SIZE_OF_WIKI / word_freq)))\n","#     return newPost\n","\n","def calculate_df(postings):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","      postings: RDD\n","        An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","      RDD\n","        An RDD where each element is a (token, df) pair.\n","    '''\n","    # Count the document frequency for each token\n","    return postings.map(lambda x: (x[0], len(x[1])))\n","\n","def partition_postings_and_write(postings, folder_name):\n","    ''' A function that partitions the posting lists into buckets, writes out\n","    all posting lists in a bucket to disk, and returns the posting locations for\n","    each bucket. Partitioning should be done through the use of `token2bucket`\n","    above. Writing to disk should use the function  `write_a_posting_list`, a\n","    static method implemented in inverted_index_colab.py under the InvertedIndex\n","    class.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and\n","      offsets its posting list was written to. See `write_a_posting_list` for\n","      more details.\n","    '''\n","    # posting in postings is word and posting list tuple\n","    bucketed_postings = postings.map(lambda x: (token2bucket_id(x[0]), x))\n","    # Group postings by bucket ID\n","    grouped_postings = bucketed_postings.groupByKey()\n","    # Write each bucket's postings to disk and collect locations\n","    posting_locations = grouped_postings.map(lambda x: InvertedIndex.write_a_posting_list(x, folder_name, bucket_name))\n","\n","    return posting_locations"]},{"cell_type":"code","execution_count":7,"id":"55c8764e","metadata":{"id":"0b5d7296","nbgrader":{"grade":false,"grade_id":"cell-index_construction","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def create_posting_locs(doc_text_pairs, folder_name, Biagrm, Stem, filter_thershold):\n","    # word counts map\n","    print(f'started to create posting locs in folder_name: {folder_name}')\n","    tokenized_text_pairs = doc_text_pairs.map(lambda text_id_pair: (tokenize_text(text_id_pair[0], Biagrm, Stem), text_id_pair[1]))\n","    print(f'finished tokenizing in folder_name: {folder_name}')\n","    word_counts = tokenized_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","    print(f'finished word countes in folder_name: {folder_name}')\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    print(f'created postings in folder_name: {folder_name}')\n","    postings_filtered = postings.filter(lambda x: len(x[1]) > filter_thershold)\n","    print(f'filtered postings in folder_name: {folder_name}')\n","    # partition posting lists and write out\n","    partition_postings_and_write(postings_filtered, folder_name)\n","    print(f'finished writing to disk in folder: {folder_name}')\n","    doc_count_res = tokenized_text_pairs.map(lambda tok_text_pair: (tok_text_pair[1], len(tok_text_pair[0])))\n","    doc_len_map = doc_count_res.collectAsMap()\n","    print(f'finished creating doc_len_map in folder: {folder_name}')\n","    w2df = calculate_df(postings_filtered)\n","    w2df_dict = w2df.collectAsMap()\n","    print(f'finished w2df in folder: {folder_name}')\n","    return w2df_dict, doc_len_map\n","    "]},{"cell_type":"code","execution_count":8,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def read_posting_locs(folder_name):\n","    # collect all posting lists locations into one super-set\n","    super_posting_locs = defaultdict(list)\n","    for blob in client.list_blobs(bucket_name, prefix=f'{folder_name}'):\n","        if not blob.name.endswith(\"pickle\"):\n","            continue\n","        with blob.open(\"rb\") as f:\n","            posting_locs = pickle.load(f)\n","            for k, v in posting_locs.items():\n","                super_posting_locs[k].extend(v)\n","    return super_posting_locs"]},{"cell_type":"markdown","id":"f6f66e3a","metadata":{"id":"VhAV0A6dNZWY"},"source":["# Putting it all together"]},{"cell_type":"code","execution_count":9,"id":"f4c87b4d","metadata":{},"outputs":[],"source":["def create_posting_map(super_posting_locs, w2df_dict, inverted_name, doc_len_map):\n","    # Create inverted index instance\n","    inverted = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    inverted.posting_locs = super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","    inverted.df = w2df_dict\n","    #doc len dic\n","    inverted.dl = doc_len_map\n","    # write the global stats out\n","    inverted.write_index('.', inverted_name)\n","    # upload to gs\n","    index_src = f\"{inverted_name}.pkl\"\n","    index_dst = f'gs://{bucket_name}/inverted_indices/{index_src}'\n","    !gsutil cp $index_src $index_dst\n","    return inverted"]},{"cell_type":"code","execution_count":10,"id":"a5d2cfb6","metadata":{"id":"54vqT_0WNc3w"},"outputs":[],"source":["def create_invertedIndex(doc_text_pairs, folder_name, Biagram, Stem, filter_thershold):\n","    w2df_dict, doc_len_map = create_posting_locs(doc_text_pairs, folder_name, Biagram, Stem, filter_thershold)\n","    super_posting_locs = read_posting_locs(folder_name)\n","    return create_posting_map(super_posting_locs, w2df_dict, folder_name, doc_len_map)"]},{"cell_type":"code","execution_count":null,"id":"24f93410","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["started to create posting locs in folder_name: inverted_text_with_stemming\n","finished tokenizing in folder_name: inverted_text_with_stemming\n","finished word countes in folder_name: inverted_text_with_stemming\n","created postings in folder_name: inverted_text_with_stemming\n","filtered postings in folder_name: inverted_text_with_stemming\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 2:==================================>                     (76 + 8) / 124]\r"]}],"source":["STEMMING_BODY_FOLDER = 'inverted_text_with_stemming'\n","STEMMING_TITLE_FOLDER = 'inverted_title_with_stemming'\n","NO_STEM_TITLE_FOLDER = 'inverted_title_no_stem'\n","NO_STEM_ANCHOR_FOLDER = 'inverted_anchor_no_stem'\n","STEM_ANCHOR_FOLDER = 'inverted_anchor_stem'\n","ANCHOR_RELATION = 'anchor_relations'\n","\n","parquetFile = spark.read.parquet(*paths)\n","text_data = parquetFile.select(\"text\", \"id\").rdd  \n","inverted1 = create_invertedIndex(text_data, f'{STEMMING_BODY_FOLDER}', False, True, 50)"]},{"cell_type":"code","execution_count":null,"id":"428af0ec","metadata":{},"outputs":[],"source":["text_data = parquetFile.select(\"title\", \"id\").rdd  \n","inverted_no_stem_title = create_invertedIndex(text_data, f'{NO_STEM_TITLE_FOLDER}', False, False, 20)"]},{"cell_type":"code","execution_count":null,"id":"5804ec58","metadata":{},"outputs":[],"source":["title_data = parquetFile.select(\"title\", \"id\").rdd  \n","inverted1 = create_invertedIndex(text_data, f'{STEMMING_TITLE_FOLDER}', False, True, 20)"]},{"cell_type":"code","execution_count":null,"id":"789c16f7","metadata":{},"outputs":[],"source":["def return_ids_from_anchor_list(anchot_text):\n","    ids_list = []\n","    for tup in anchot_text:\n","        ids_list.append(tup[0])\n","    return ids_list\n","\n","\n","def return_text_from_anchor_list(anchot_text):\n","    text = ''\n","    for tup in anchot_text:\n","        text = text + ' ' + tup[1]\n","    return text\n"]},{"cell_type":"code","execution_count":null,"id":"b9c4f51a","metadata":{},"outputs":[],"source":["parquetFile = spark.read.parquet(*paths)\n","anchor_data = parquetFile.select(\"anchor_text\", \"id\").rdd \n","\n","anchor_text = anchor_data.map(lambda anchor_id_pair: (return_text_from_anchor_list(anchor_id_pair[0]), anchor_id_pair[1]))\n","create_invertedIndex(anchor_text, f'{STEM_ANCHOR_FOLDER}', False, True, 20)"]},{"cell_type":"code","execution_count":null,"id":"96e1be1c","metadata":{},"outputs":[],"source":["# print(anchor_relations)\n","# client = storage.Client()\n","\n","# # Get the bucket\n","# bucket = client.bucket(bucket_name)\n","\n","# anchor_relations_dic = anchor_relations.collectAsMap()\n","# data = pickle.dumps(anchor_relations_dic)\n","# blob = bucket.blob(\"anchor_relations.pkl\")\n","# blob.upload_from_string(data)\n","\n","# parquetFile = spark.read.parquet(*paths)\n","# anchor_data = parquetFile.select(\"anchor_text\", \"id\").rdd \n","\n","# anchor_text = anchor_data.map(lambda anchor_id_pair: (return_text_from_anchor_list(anchor_id_pair[0]), anchor_id_pair[1]))\n","# create_invertedIndex(anchor_text, f'{NO_STEM_ANCHOR_FOLDER}', False, False,  20)\n","\n","# parquetFile = spark.read.parquet(*paths)\n","# anchor_data = parquetFile.select(\"anchor_text\", \"id\").rdd \n","# anchor_relations = anchor_data.map(lambda anchor_id_pair: (anchor_id_pair[1], return_ids_from_anchor_list(anchor_id_pair[0])))"]},{"cell_type":"markdown","id":"c52dee14","metadata":{"id":"fc0667a9","nbgrader":{"grade":false,"grade_id":"cell-2a6d655c112e79c5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# PageRank"]},{"cell_type":"code","execution_count":null,"id":"31a516e2","metadata":{"id":"yVjnTvQsegc-"},"outputs":[],"source":["def generate_graph(pages):\n","    vertices = pages.flatMap(lambda page: [(page[0], )] + [(link[0], ) for link in page[1]])\n","\n","    edges = pages.flatMap(lambda page: [(page[0], link[0]) for link in page[1]])\n","\n","    edges = edges.distinct()\n","    vertices = vertices.distinct()\n","\n","    return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"6bc05ba3","metadata":{"id":"db005700","nbgrader":{"grade":false,"grade_id":"cell-PageRank","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["parquetFile = spark.read.parquet(*paths)\n","pages_links = parquetFile.select(\"id\",\"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = pr.sort(col('pagerank').desc())\n","pr.repartition(1).write.csv(f'gs://{bucket_name}/pr_full_run', compression=\"gzip\")\n","pr.show()"]},{"cell_type":"markdown","id":"7081206d","metadata":{},"source":["# PageViews"]},{"cell_type":"code","execution_count":null,"id":"0976547b","metadata":{},"outputs":[],"source":["# creating page views dictionary and saving it to bucket\n","from pathlib import Path\n","\n","pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_stem = p.stem\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n","# Download the file (2.3GB) \n","!wget -N $pv_path\n","# Filter for English pages, and keep just two fields: article ID (3) and monthly \n","# total number of page views (5). Then, remove lines with article id or page \n","# view values that are not a sequence of digits.\n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n","# Create a Counter (dictionary) that sums up the pages views for the same \n","# article, resulting in a mapping from article id to total page views.\n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","    for line in f:\n","        parts = line.split(' ')\n","        wid2pv.update({int(parts[0]): int(parts[1])})\n","# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","    pickle.dump(wid2pv, f)"]},{"cell_type":"code","execution_count":null,"id":"87724e96","metadata":{},"outputs":[],"source":["# Serialize the object using pickle\n","data = pickle.dumps(wid2pv)\n","\n","client = storage.Client()\n","\n","# Get the bucket\n","bucket = client.bucket(bucket_name)\n","\n","# Create a new GCS object with the serialized data\n","blob = bucket.blob(\"wid2pv.pkl\")\n","blob.upload_from_string(data)"]},{"cell_type":"code","execution_count":null,"id":"9aa2518d","metadata":{},"outputs":[],"source":["# Save doc id to title dict\n","text_data = parquetFile.select(\"id\", \"title\").rdd\n","titles_dict = text_data.collectAsMap()\n","data = pickle.dumps(titles_dict)\n","blob = bucket.blob(\"id2title.pkl\")\n","blob.upload_from_string(data)"]},{"cell_type":"markdown","id":"b75e559e","metadata":{},"source":["###### MOST COMMON"]},{"cell_type":"code","execution_count":null,"id":"f4fde02b","metadata":{},"outputs":[],"source":["# def create_new_inverted(old_inverted, most_common_map, inverted_name):\n","#     inverted = InvertedIndex()\n","#     inverted.posting_locs = old_inverted.posting_locs\n","#     inverted.df = old_inverted.df\n","#     inverted.dl = old_inverted.dl\n","#     inverted.mcw = most_common_map\n","#     inverted.write_index('.', inverted_name)\n","#     index_src = f\"{inverted_name}.pkl\"\n","#     index_dst = f'gs://{bucket_name}/inverted_indices/{index_src}'\n","#     !gsutil cp $index_src $index_dst\n","#     return inverted"]},{"cell_type":"code","execution_count":null,"id":"a2a36708","metadata":{},"outputs":[],"source":["# folder_path = 'inverted_indices'\n","# def fetch_curr_inverted_index(index_name):\n","#     pickle_index = InvertedIndex.read_index(folder_path, index_name, bucket_name)\n","#     return pickle_index"]},{"cell_type":"code","execution_count":null,"id":"227261fc","metadata":{},"outputs":[],"source":["# STEMMING_BODY_FOLDER = 'inverted_text_with_stemming'\n","# BIGRAM_BODY_FOLDER = 'inverted_text_with_bigram'\n","# STEMMING_TITLE_FOLDER = 'inverted_title_with_stemming'\n","# BIGRAM_TITLE_FOLDER = 'inverted_title_with_bigram'\n","# STEMMING_MOST_COMMON_INVERTED_TEXT = 'inverted_text_with_stemming_two'\n","# BIGRAM_MOST_COMMON_INVERTED_TEXT = 'inverted_text_with_bigram_two'\n","# STEMMING_MOST_COMMON_INVERTED_TITLE = 'inverted_title_with_stemming_two'\n","# BIGRAM_MOST_COMMON_INVERTED_TITLE = 'inverted_title_with_bigram_two'\n","\n","\n","# def find_most_common_word(text, biagram):\n","#     tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","#     tokens = [tok for tok in tokens if tok not in all_stopwords]\n","#     tokens = [stemmer.stem(token) for token in tokens]\n","#     if biagram:\n","#         tokens = list(ngrams(tokens, 2))\n","#         tokens = [' '.join(bigram) for bigram in tokens]   \n","#     if tokens:\n","#         most_common_token = Counter(tokens).most_common(1)[0][1]\n","#         return most_common_token\n","#     return 0 \n","\n","# #Save doc id to most common word to title dict\n","# doc_data = parquetFile.select(\"id\", \"text\").rdd  \n","# print(\"finished reading data\")\n","# doc_to_most_common = doc_data.mapValues(lambda text: find_most_common_word(text, False))\n","# print(\"mapping doc to most common\")\n","# titles_dict = doc_to_most_common.collectAsMap()\n","# print(\"converted to map\")\n","# curr_inverted = fetch_curr_inverted_index(STEMMING_BODY_FOLDER)\n","# new_inverted = create_new_inverted(curr_inverted, titles_dict, STEMMING_MOST_COMMON_INVERTED_TEXT)\n","# print(len(new_inverted.mcw))"]},{"cell_type":"code","execution_count":null,"id":"fd4dd80a","metadata":{},"outputs":[],"source":["# STEMMING_MOST_COMMON_INVERTED_TITLE = 'inverted_title_with_stemming_two'\n","# doc_data = parquetFile.select(\"id\", \"title\").rdd  \n","# doc_to_most_common = doc_data.mapValues(lambda text: find_most_common_word(text, False))\n","# titles_dict = doc_to_most_common.collectAsMap()\n","# curr_inverted = fetch_curr_inverted_index(STEMMING_TITLE_FOLDER)\n","# new inverted = create_new_inverted(curr_inverted, titles_dict, STEMMING_MOST_COMMON_INVERTED_TITLE)"]},{"cell_type":"code","execution_count":null,"id":"9eee335b","metadata":{},"outputs":[],"source":["# doc_data = parquetFile.select(\"id\", \"text\").rdd  \n","# doc_to_most_common = doc_data.mapValues(lambda text: find_most_common_word(text, True))\n","# titles_dict = doc_to_most_common.collectAsMap()\n","# curr_inverted = fetch_curr_inverted_index(BIGRAM_BODY_FOLDER)\n","# new inverted = create_new_inverted(curr_inverted, titles_dict, BIGRAM_MOST_COMMON_INVERTED_TEXT)"]},{"cell_type":"code","execution_count":null,"id":"7bee108f","metadata":{},"outputs":[],"source":["# doc_data = parquetFile.select(\"id\", \"title\").rdd  \n","# doc_to_most_common = doc_data.mapValues(lambda text: find_most_common_word(text, True))\n","# titles_dict = doc_to_most_common.collectAsMap()\n","# curr_inverted = fetch_curr_inverted_index(BIGRAM_TITLE_FOLDER)\n","# new inverted = create_new_inverted(curr_inverted, titles_dict, BIGRAM_MOST_COMMON_INVERTED_TITLE)"]},{"cell_type":"code","execution_count":7,"id":"17597365","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AccessDeniedException: 403 There is an account problem for the requested project.\r\n"]}],"source":["old_bucket = 'tfidf_bucket_318437159'\n","STEMMING_TITLE_FOLDER = 'inverted_title_with_stemming'\n","index_src = f\"{STEMMING_TITLE_FOLDER}.pkl\"\n","\n","index_src = f'gs://{old_bucket}/inverted_indices/{index_src}'\n","index_dst = f'gs://{bucket_name}/inverted_indices/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"23aea7f4","metadata":{},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}